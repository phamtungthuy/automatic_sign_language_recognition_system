{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Sign Language Recognition - Model Serving on Colab\n",
        "\n",
        "Notebook n√†y gi√∫p b·∫°n deploy model ƒë√£ trained l√™n Colab v√† expose ra public URL qua ngrok."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive ƒë·ªÉ l·∫•y model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install fastapi uvicorn python-multipart pyngrok nest-asyncio -q\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è QUAN TR·ªåNG: ƒêƒÉng k√Ω t√†i kho·∫£n ngrok mi·ªÖn ph√≠ t·∫°i https://ngrok.com\n",
        "# Sau ƒë√≥ l·∫•y authtoken v√† paste v√†o ƒë√¢y\n",
        "NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN\"  # <-- THAY TOKEN C·ª¶A B·∫†N V√ÄO ƒê√ÇY\n",
        "\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "print(\"‚úÖ Ngrok configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Model Architecture (same as training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import math\n",
        "from torchvision import models\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "NUM_CLASSES = 100\n",
        "TARGET_FRAMES = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_weights = self.attention(x)\n",
        "        attn_weights = F.softmax(attn_weights, dim=1)\n",
        "        pooled = torch.sum(attn_weights * x, dim=1)\n",
        "        return pooled\n",
        "\n",
        "\n",
        "class ConvNeXtTransformer(nn.Module):\n",
        "    def __init__(self, num_classes=100, hidden_size=256, resnet_pretrained_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        convnext = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "        self.cnn = convnext.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.feature_dim = 768\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=self.feature_dim,\n",
        "            max_len=64,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.feature_dim,\n",
        "            nhead=8,\n",
        "            dim_feedforward=self.feature_dim * 4,\n",
        "            dropout=0.3,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        self.attention_pool = AttentionPooling(self.feature_dim)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.LayerNorm(self.feature_dim),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(self.feature_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.transformer.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "        for m in self.attention_pool.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.cnn(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(B, T, self.feature_dim)\n",
        "\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.attention_pool(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Model & Label Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ==========\n",
        "# Thay ƒë·ªïi c√°c ƒë∆∞·ªùng d·∫´n n√†y theo v·ªã tr√≠ file c·ªßa b·∫°n tr√™n Google Drive\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/OlympicAI/augmented_balanced_convnexttransformer_best_model.pth\"\n",
        "LABEL_MAPPING_PATH = \"/content/drive/MyDrive/OlympicAI/dataset/label_mapping.pkl\"\n",
        "\n",
        "# =========================================\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = ConvNeXtTransformer(num_classes=NUM_CLASSES, hidden_size=256)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "print(f\"‚úÖ Model loaded from {MODEL_PATH}\")\n",
        "\n",
        "# Load label mapping\n",
        "with open(LABEL_MAPPING_PATH, 'rb') as f:\n",
        "    label_mapping = pickle.load(f)\n",
        "idx_to_label = {v: k for k, v in label_mapping.items()}\n",
        "print(f\"‚úÖ Loaded {len(idx_to_label)} classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Video Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "def read_video_bytes(video_bytes):\n",
        "    \"\"\"Read video from bytes (uploaded file)\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:\n",
        "        tmp.write(video_bytes)\n",
        "        tmp_path = tmp.name\n",
        "    \n",
        "    cap = cv2.VideoCapture(tmp_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    os.unlink(tmp_path)  # Delete temp file\n",
        "    \n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(\"Could not read any frames from video\")\n",
        "    return torch.from_numpy(np.stack(frames, axis=0))\n",
        "\n",
        "\n",
        "def downsample_frames(frames, target_frames=TARGET_FRAMES):\n",
        "    \"\"\"Sample target_frames from video\"\"\"\n",
        "    total = frames.shape[0]\n",
        "    if total >= target_frames:\n",
        "        indices = torch.linspace(0, total - 1, target_frames).long()\n",
        "    else:\n",
        "        indices = torch.arange(total)\n",
        "        pad = target_frames - total\n",
        "        indices = torch.cat([indices, indices[-1].repeat(pad)])\n",
        "\n",
        "    frames = frames[indices]\n",
        "\n",
        "    # Resize to 224x224 if needed\n",
        "    if frames.shape[1] != 224 or frames.shape[2] != 224:\n",
        "        frames = frames.permute(0, 3, 1, 2).float()\n",
        "        frames = F.interpolate(frames, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        frames = frames.permute(0, 2, 3, 1).to(torch.uint8)\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "def normalize_frames(frames):\n",
        "    \"\"\"Normalize to ImageNet mean/std\"\"\"\n",
        "    frames = frames.float() / 255.0\n",
        "    frames = frames.permute(0, 3, 1, 2)  # (T, H, W, C) -> (T, C, H, W)\n",
        "\n",
        "    mean = torch.tensor(MEAN).view(1, 3, 1, 1)\n",
        "    std = torch.tensor(STD).view(1, 3, 1, 1)\n",
        "    frames = (frames - mean) / std\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "def preprocess_video(video_bytes):\n",
        "    \"\"\"Full preprocessing pipeline\"\"\"\n",
        "    frames = read_video_bytes(video_bytes)\n",
        "    frames = downsample_frames(frames)\n",
        "    frames = normalize_frames(frames)\n",
        "    return frames.unsqueeze(0)  # Add batch dim: (1, T, C, H, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create FastAPI Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow running in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"ü§ü Sign Language Recognition API\",\n",
        "    description=\"API for Vietnamese Sign Language Recognition using ConvNeXt-Transformer\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Enable CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "# Response models\n",
        "class PredictionResponse(BaseModel):\n",
        "    label: str\n",
        "    confidence: float\n",
        "    label_idx: int\n",
        "\n",
        "\n",
        "class TopKPrediction(BaseModel):\n",
        "    label: str\n",
        "    confidence: float\n",
        "    label_idx: int\n",
        "\n",
        "\n",
        "class PredictionResponseTopK(BaseModel):\n",
        "    predictions: List[TopKPrediction]\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"message\": \"ü§ü Sign Language Recognition API is running!\",\n",
        "        \"docs\": \"/docs\",\n",
        "        \"endpoints\": {\n",
        "            \"predict\": \"POST /predict\",\n",
        "            \"predict_topk\": \"POST /predict/topk?k=5\",\n",
        "            \"health\": \"GET /health\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"model_loaded\": True,\n",
        "        \"device\": DEVICE,\n",
        "        \"num_classes\": NUM_CLASSES\n",
        "    }\n",
        "\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "async def predict(file: UploadFile = File(...)):\n",
        "    \"\"\"\n",
        "    Upload a video file and get the predicted sign language label.\n",
        "    \n",
        "    Supported formats: mp4, avi, mov, mkv\n",
        "    \"\"\"\n",
        "    # Validate file type\n",
        "    if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
        "        raise HTTPException(status_code=400, detail=\"Invalid file format. Use mp4, avi, mov, or mkv\")\n",
        "    \n",
        "    try:\n",
        "        # Read video bytes\n",
        "        video_bytes = await file.read()\n",
        "        \n",
        "        # Preprocess\n",
        "        frames = preprocess_video(video_bytes)\n",
        "        frames = frames.to(DEVICE)\n",
        "        \n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            outputs = model(frames)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            confidence, predicted = probs.max(1)\n",
        "            \n",
        "            label_idx = predicted.item()\n",
        "            label_name = idx_to_label[label_idx]\n",
        "            conf = confidence.item()\n",
        "        \n",
        "        return PredictionResponse(\n",
        "            label=label_name,\n",
        "            confidence=round(conf, 4),\n",
        "            label_idx=label_idx\n",
        "        )\n",
        "    \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing video: {str(e)}\")\n",
        "\n",
        "\n",
        "@app.post(\"/predict/topk\", response_model=PredictionResponseTopK)\n",
        "async def predict_topk(file: UploadFile = File(...), k: int = 5):\n",
        "    \"\"\"\n",
        "    Upload a video file and get top-k predicted sign language labels.\n",
        "    \n",
        "    - **k**: Number of top predictions to return (default: 5)\n",
        "    \"\"\"\n",
        "    if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
        "        raise HTTPException(status_code=400, detail=\"Invalid file format. Use mp4, avi, mov, or mkv\")\n",
        "    \n",
        "    try:\n",
        "        video_bytes = await file.read()\n",
        "        frames = preprocess_video(video_bytes)\n",
        "        frames = frames.to(DEVICE)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(frames)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            top_probs, top_indices = torch.topk(probs, k=min(k, NUM_CLASSES), dim=1)\n",
        "            \n",
        "            predictions = []\n",
        "            for prob, idx in zip(top_probs[0], top_indices[0]):\n",
        "                predictions.append(TopKPrediction(\n",
        "                    label=idx_to_label[idx.item()],\n",
        "                    confidence=round(prob.item(), 4),\n",
        "                    label_idx=idx.item()\n",
        "                ))\n",
        "        \n",
        "        return PredictionResponseTopK(predictions=predictions)\n",
        "    \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing video: {str(e)}\")\n",
        "\n",
        "\n",
        "@app.get(\"/labels\")\n",
        "async def get_labels():\n",
        "    \"\"\"Get list of all available sign language labels\"\"\"\n",
        "    return {\n",
        "        \"total_classes\": NUM_CLASSES,\n",
        "        \"labels\": list(label_mapping.keys())\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Start Server with Ngrok Tunnel üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start ngrok tunnel\n",
        "PORT = 8000\n",
        "public_url = ngrok.connect(PORT)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ SERVER IS RUNNING!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüåê Public URL: {public_url}\")\n",
        "print(f\"üìö API Docs:   {public_url}/docs\")\n",
        "print(f\"üìä Health:     {public_url}/health\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìù Usage Example (with curl):\")\n",
        "print(f'   curl -X POST \"{public_url}/predict\" -F \"file=@your_video.mp4\"')\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚ö†Ô∏è  Keep this cell running! Press Ctrl+C to stop.\")\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the server (this will block - keep running!)\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=PORT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Alternative: Use localtunnel (if ngrok doesn't work)\n",
        "\n",
        "N·∫øu ngrok kh√¥ng ho·∫°t ƒë·ªông, b·∫°n c√≥ th·ªÉ d√πng localtunnel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Uncomment n·∫øu mu·ªën d√πng localtunnel thay v√¨ ngrok\n",
        "# !npm install -g localtunnel\n",
        "# \n",
        "# # Ch·∫°y server trong background\n",
        "# import subprocess\n",
        "# subprocess.Popen([\"python\", \"-c\", f\"\"\"\n",
        "# import uvicorn\n",
        "# uvicorn.run(app, host='0.0.0.0', port={PORT})\n",
        "# \"\"\"])\n",
        "# \n",
        "# # T·∫°o tunnel\n",
        "# !lt --port 8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Test API (Optional)\n",
        "\n",
        "Ch·∫°y cell n√†y t·ª´ m·ªôt notebook kh√°c ho·∫∑c terminal ƒë·ªÉ test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Test code - ch·∫°y sau khi server ƒë√£ start\n",
        "# import requests\n",
        "# \n",
        "# API_URL = \"YOUR_NGROK_URL\"  # Paste URL t·ª´ output ·ªü tr√™n\n",
        "# \n",
        "# # Health check\n",
        "# response = requests.get(f\"{API_URL}/health\")\n",
        "# print(\"Health:\", response.json())\n",
        "# \n",
        "# # Predict\n",
        "# with open(\"test_video.mp4\", \"rb\") as f:\n",
        "#     response = requests.post(\n",
        "#         f\"{API_URL}/predict\",\n",
        "#         files={\"file\": f}\n",
        "#     )\n",
        "# print(\"Prediction:\", response.json())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
