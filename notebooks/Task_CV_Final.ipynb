{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "6JkuYbtvLCdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iwUNpCdALA4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4230d661-f780-48ec-df0a-1759fca8faf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/OlympicAI"
      ],
      "metadata": {
        "id": "XEzPcEpBLUXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee3080f-b934-4911-fc4a-d463e73253cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/OlympicAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "kq_x4HFwLAN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf488cdd-3211-4e3d-f16c-09c007bc7f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline.ipynb\tdataset.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "2HiQKb4AG0Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "IZFDZsFXLiQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 100\n",
        "TARGET_FRAMES = 16"
      ],
      "metadata": {
        "id": "sEthIGjuh4JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_video(video_path):\n",
        "    \"\"\"Read video frames using OpenCV\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(f\"Could not read any frames from {video_path}\")\n",
        "    frames = torch.from_numpy(np.stack(frames, axis=0))\n",
        "    return frames\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for batching\"\"\"\n",
        "    frames = torch.stack([item['frames'] for item in batch])\n",
        "    labels = torch.tensor([item['label_idx'] for item in batch])\n",
        "    label_names = [item['label'] for item in batch]\n",
        "    return {'frames': frames, 'label_idx': labels, 'label': label_names}"
      ],
      "metadata": {
        "id": "TgQWpjOHLvEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VideoAugmentation:\n",
        "    \"\"\"\n",
        "    Augmentation cho video - CONSISTENT across all frames\n",
        "    Chỉ dùng cho training, không dùng cho val/test\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 crop_scale=(0.85, 1.0),\n",
        "                 brightness=0.2,\n",
        "                 contrast=0.2,\n",
        "                 saturation=0.2,\n",
        "                 speed_range=(0.9, 1.1)):\n",
        "\n",
        "        self.crop_scale = crop_scale\n",
        "        self.brightness = brightness\n",
        "        self.contrast = contrast\n",
        "        self.saturation = saturation\n",
        "        self.speed_range = speed_range\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            frames: tensor (T, H, W, C) với giá trị 0-255\n",
        "        Returns:\n",
        "            augmented frames: tensor (T, H, W, C)\n",
        "        \"\"\"\n",
        "        # 1. Speed Augmentation (thay đổi số frames)\n",
        "        frames = self._speed_augment(frames)\n",
        "\n",
        "        # 2. Random Resized Crop (CONSISTENT cho tất cả frames)\n",
        "        frames = self._random_resized_crop(frames)\n",
        "\n",
        "        # 3. Color Jitter (CONSISTENT cho tất cả frames)\n",
        "        frames = self._color_jitter(frames)\n",
        "\n",
        "        return frames\n",
        "\n",
        "    def _speed_augment(self, frames):\n",
        "        \"\"\"Thay đổi tốc độ video bằng cách resample frames\"\"\"\n",
        "        T = frames.shape[0]\n",
        "        speed = random.uniform(self.speed_range[0], self.speed_range[1])\n",
        "\n",
        "        new_T = int(T / speed)\n",
        "        if new_T < 4:\n",
        "            new_T = 4\n",
        "        if new_T == T:\n",
        "            return frames\n",
        "\n",
        "        # Resample frames\n",
        "        indices = torch.linspace(0, T - 1, new_T).long()\n",
        "        indices = torch.clamp(indices, 0, T - 1)\n",
        "        frames = frames[indices]\n",
        "\n",
        "        return frames\n",
        "\n",
        "    def _random_resized_crop(self, frames):\n",
        "        \"\"\"Random crop rồi resize về 224x224 - CONSISTENT\"\"\"\n",
        "        T, H, W, C = frames.shape\n",
        "\n",
        "        # Random scale và position (CÙNG cho tất cả frames)\n",
        "        scale = random.uniform(self.crop_scale[0], self.crop_scale[1])\n",
        "        crop_h, crop_w = int(H * scale), int(W * scale)\n",
        "\n",
        "        top = random.randint(0, H - crop_h)\n",
        "        left = random.randint(0, W - crop_w)\n",
        "\n",
        "        # Crop tất cả frames GIỐNG NHAU\n",
        "        frames = frames[:, top:top+crop_h, left:left+crop_w, :]\n",
        "\n",
        "        # Resize về 224x224\n",
        "        # (T, H, W, C) -> (T, C, H, W) for interpolate\n",
        "        frames = frames.permute(0, 3, 1, 2).float()\n",
        "        frames = F.interpolate(frames, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        # (T, C, H, W) -> (T, H, W, C)\n",
        "        frames = frames.permute(0, 2, 3, 1)\n",
        "\n",
        "        return frames.to(torch.uint8)\n",
        "\n",
        "    def _color_jitter(self, frames):\n",
        "        \"\"\"Color jitter - CONSISTENT cho tất cả frames\"\"\"\n",
        "        # Random parameters (CÙNG cho tất cả frames)\n",
        "        brightness_factor = 1.0 + random.uniform(-self.brightness, self.brightness)\n",
        "        contrast_factor = 1.0 + random.uniform(-self.contrast, self.contrast)\n",
        "        saturation_factor = 1.0 + random.uniform(-self.saturation, self.saturation)\n",
        "\n",
        "        frames = frames.float()\n",
        "\n",
        "        # Brightness\n",
        "        frames = frames * brightness_factor\n",
        "\n",
        "        # Contrast\n",
        "        mean = frames.mean(dim=(1, 2), keepdim=True)\n",
        "        frames = (frames - mean) * contrast_factor + mean\n",
        "\n",
        "        # Saturation\n",
        "        gray = frames.mean(dim=-1, keepdim=True)\n",
        "        frames = gray + (frames - gray) * saturation_factor\n",
        "\n",
        "        # Clamp to valid range\n",
        "        frames = torch.clamp(frames, 0, 255)\n",
        "\n",
        "        return frames.to(torch.uint8)\n"
      ],
      "metadata": {
        "id": "ISBMaKiliCUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, label_to_idx_path, transform=None,\n",
        "                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n",
        "                 target_frames=16, training=False):\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.mean, self.std = mean, std\n",
        "        self.target_frames = target_frames\n",
        "        self.training = training\n",
        "\n",
        "        # Augmentation chỉ khi training\n",
        "        self.augmentation = VideoAugmentation() if training else None\n",
        "\n",
        "        self.instances, self.labels, self.label_idx = [], [], []\n",
        "\n",
        "        with open(label_to_idx_path, 'rb') as f:\n",
        "            self.label_mapping = pickle.load(f)\n",
        "\n",
        "        for label_folder in sorted(os.listdir(root_dir))[:NUM_CLASSES]:\n",
        "            path = os.path.join(root_dir, label_folder)\n",
        "            if os.path.isdir(path):\n",
        "                for video_file in os.listdir(path):\n",
        "                    video_path = os.path.join(path, video_file)\n",
        "                    self.instances.append(video_path)\n",
        "                    self.labels.append(label_folder)\n",
        "                    self.label_idx.append(self.label_mapping[label_folder])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.instances[idx]\n",
        "        frames = read_video(video_path)\n",
        "\n",
        "        # ============ AUGMENTATION (chỉ khi training) ============\n",
        "        if self.training and self.augmentation is not None:\n",
        "            frames = self.augmentation(frames)\n",
        "        # =========================================================\n",
        "\n",
        "        frames = self._downsample_frames(frames)\n",
        "        frames = self._normalize(frames)\n",
        "\n",
        "        return {\n",
        "            'frames': frames,\n",
        "            'label_idx': self.label_idx[idx],\n",
        "            'label': self.labels[idx]\n",
        "        }\n",
        "\n",
        "    def _downsample_frames(self, frames):\n",
        "        \"\"\"Lấy target_frames từ video\"\"\"\n",
        "        total = frames.shape[0]\n",
        "        if total >= self.target_frames:\n",
        "            indices = torch.linspace(0, total - 1, self.target_frames).long()\n",
        "        else:\n",
        "            indices = torch.arange(total)\n",
        "            pad = self.target_frames - total\n",
        "            indices = torch.cat([indices, indices[-1].repeat(pad)])\n",
        "\n",
        "        frames = frames[indices]\n",
        "\n",
        "        # Resize về 224x224 nếu chưa\n",
        "        if frames.shape[1] != 224 or frames.shape[2] != 224:\n",
        "            frames = frames.permute(0, 3, 1, 2).float()\n",
        "            frames = F.interpolate(frames, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            frames = frames.permute(0, 2, 3, 1).to(torch.uint8)\n",
        "\n",
        "        return frames\n",
        "\n",
        "    def _normalize(self, frames):\n",
        "        \"\"\"Normalize về ImageNet mean/std\"\"\"\n",
        "        frames = frames.float() / 255.0\n",
        "        frames = frames.permute(0, 3, 1, 2)  # (T, H, W, C) -> (T, C, H, W)\n",
        "\n",
        "        mean = torch.tensor(self.mean).view(1, 3, 1, 1)\n",
        "        std = torch.tensor(self.std).view(1, 3, 1, 1)\n",
        "        frames = (frames - mean) / std\n",
        "\n",
        "        return frames"
      ],
      "metadata": {
        "id": "iwTvEfzViD65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_balanced_sampler(dataset):\n",
        "    \"\"\"Create balanced sampler for imbalanced dataset\"\"\"\n",
        "    if hasattr(dataset, 'dataset'):\n",
        "        all_labels = [dataset.dataset.label_idx[i] for i in dataset.indices]\n",
        "    else:\n",
        "        all_labels = dataset.label_idx\n",
        "\n",
        "    class_counts = np.bincount(all_labels)\n",
        "    class_weights = 1.0 / class_counts\n",
        "    sample_weights = [class_weights[label] for label in all_labels]\n",
        "    sample_weights = torch.FloatTensor(sample_weights)\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    print(f\"Balanced Sampler: class counts min={class_counts.min()}, max={class_counts.max()}\")\n",
        "    return sampler"
      ],
      "metadata": {
        "id": "SuPUusFmiFMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding cho temporal sequence\"\"\"\n",
        "    def __init__(self, d_model, max_len=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, d_model)\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    \"\"\"Attention pooling thay cho last hidden của LSTM\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, dim)\n",
        "        attn_weights = self.attention(x)  # (B, T, 1)\n",
        "        attn_weights = F.softmax(attn_weights, dim=1)\n",
        "        pooled = torch.sum(attn_weights * x, dim=1)  # (B, dim)\n",
        "        return pooled"
      ],
      "metadata": {
        "id": "QpQJOTl5LzcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ConvNeXtTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    ConvNeXt-Tiny + Transformer\n",
        "\n",
        "    Input:  (B, T, C, H, W) = (B, 16, 3, 224, 224)\n",
        "    Output: (B, num_classes) = (B, 100)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=100, hidden_size=256, resnet_pretrained_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. ConvNeXt-Tiny Backbone\n",
        "        convnext = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "        self.cnn = convnext.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # ConvNeXt-Tiny output = 768\n",
        "        self.feature_dim = 768\n",
        "\n",
        "        # 2. Positional Encoding\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=self.feature_dim,\n",
        "            max_len=64,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # 3. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.feature_dim,\n",
        "            nhead=8,\n",
        "            dim_feedforward=self.feature_dim * 4,\n",
        "            dropout=0.3,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        # 4. Attention Pooling\n",
        "        self.attention_pool = AttentionPooling(self.feature_dim)\n",
        "\n",
        "        # 5. Classifier\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.LayerNorm(self.feature_dim),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(self.feature_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.transformer.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "        for m in self.attention_pool.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # CNN: (B, T, C, H, W) → (B, T, 768)\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.cnn(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(B, T, self.feature_dim)\n",
        "\n",
        "        # Transformer: (B, T, 768) → (B, T, 768)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Pooling: (B, T, 768) → (B, 768)\n",
        "        x = self.attention_pool(x)\n",
        "\n",
        "        # Classifier: (B, 768) → (B, num_classes)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "E1nYSv-ph-9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate trained model on test set\n",
        "def evaluate(model, folder_path, label_to_idx_path, output_csv=\"predictions.csv\",\n",
        "             device='cuda', model_path=None, target_frames=16):\n",
        "    # Load trained weights if provided\n",
        "    if model_path:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"Loaded model from {model_path}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load label mapping\n",
        "    with open(label_to_idx_path, 'rb') as f:\n",
        "        label_mapping = pickle.load(f)\n",
        "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "    # Collect video files\n",
        "    video_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))])\n",
        "    print(f\"Found {len(video_files)} videos in '{folder_path}'\")\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    dataset = VideoDataset(\n",
        "                  root_dir=folder_path,\n",
        "                  label_to_idx_path=label_to_idx_path,\n",
        "                  target_frames=target_frames\n",
        "              )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for video_file in tqdm(video_files, desc=\"Predicting\"):\n",
        "            video_path = os.path.join(folder_path, video_file)\n",
        "            try:\n",
        "                # Read and preprocess video\n",
        "                frames = read_video(video_path)\n",
        "                frames = dataset._downsample_frames(frames)\n",
        "                frames = dataset._normalize(frames)\n",
        "                frames = frames.unsqueeze(0).to(device)  # (1, T, C, H, W)\n",
        "\n",
        "                # Predict\n",
        "                outputs = model(frames)\n",
        "                _, predicted = outputs.max(1)\n",
        "                label_idx = predicted.item()\n",
        "                label_name = idx_to_label[label_idx]\n",
        "\n",
        "                predictions.append((video_file, label_name))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {video_file}: {e}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    with open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['video_name', 'label'])\n",
        "        writer.writerows(predictions)\n",
        "\n",
        "    print(f\"\\nPredictions saved to '{output_csv}'\")\n",
        "    print(f\"Total videos processed: {len(predictions)}\")"
      ],
      "metadata": {
        "id": "AZeqonrGRci1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device='cuda'):\n",
        "    \"\"\"One training epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress = tqdm(dataloader, desc='Training')\n",
        "    for batch in progress:\n",
        "        frames, labels = batch['frames'].to(device), batch['label_idx'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        progress.set_postfix({'loss': f'{total_loss / (len(progress)+1e-9):.4f}'})\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device='cuda'):\n",
        "    \"\"\"Validation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, preds, labels_all = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc='Validation'):\n",
        "            frames, labels = batch['frames'].to(device), batch['label_idx'].to(device)\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            preds.extend(predicted.cpu().numpy())\n",
        "            labels_all.extend(labels.cpu().numpy())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels_all, preds, average='macro', zero_division=0)\n",
        "    return total_loss / len(dataloader), {'precision': precision*100, 'recall': recall*100, 'f1': f1*100}"
      ],
      "metadata": {
        "id": "5jAwVf9BiIe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader,\n",
        "                num_epochs=20, lr=1e-4, device='cuda', save_path='best_model.pth'):\n",
        "    \"\"\"Full training loop with validation and test evaluation\"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, 'min', factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_metrics = validate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Val F1: {val_metrics['f1']:.2f}% | Precision: {val_metrics['precision']:.2f}% | Recall: {val_metrics['recall']:.2f}%\")\n",
        "\n",
        "        if val_metrics['f1'] > best_f1:\n",
        "            best_f1 = val_metrics['f1']\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"✓ Best model saved with F1: {best_f1:.2f}%\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Tp8ALQlViKpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, folder_path, label_to_idx_path, output_csv=\"predictions.csv\",\n",
        "             device='cuda', model_path=None, target_frames=16):\n",
        "    \"\"\"Evaluate trained model on test set\"\"\"\n",
        "    # Load trained weights if provided\n",
        "    if model_path:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"Loaded model from {model_path}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load label mapping\n",
        "    with open(label_to_idx_path, 'rb') as f:\n",
        "        label_mapping = pickle.load(f)\n",
        "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "    # Collect video files\n",
        "    video_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))])\n",
        "    print(f\"Found {len(video_files)} videos in '{folder_path}'\")\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    dataset = VideoDataset(\n",
        "        root_dir=folder_path,\n",
        "        label_to_idx_path=label_to_idx_path,\n",
        "        target_frames=target_frames\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for video_file in tqdm(video_files, desc=\"Predicting\"):\n",
        "            video_path = os.path.join(folder_path, video_file)\n",
        "            try:\n",
        "                # Read and preprocess video\n",
        "                frames = read_video(video_path)\n",
        "                frames = dataset._downsample_frames(frames)\n",
        "                frames = dataset._normalize(frames)\n",
        "                frames = frames.unsqueeze(0).to(device)  # (1, T, C, H, W)\n",
        "\n",
        "                # Predict\n",
        "                outputs = model(frames)\n",
        "                _, predicted = outputs.max(1)\n",
        "                label_idx = predicted.item()\n",
        "                label_name = idx_to_label[label_idx]\n",
        "\n",
        "                predictions.append((video_file, label_name))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {video_file}: {e}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    with open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['video_name', 'label'])\n",
        "        writer.writerows(predictions)\n",
        "\n",
        "    print(f\"\\nPredictions saved to '{output_csv}'\")\n",
        "    print(f\"Total videos processed: {len(predictions)}\")"
      ],
      "metadata": {
        "id": "TJRaCkZSiM7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo datasets\n",
        "train_dataset_base = VideoDataset(\n",
        "    'dataset/train',\n",
        "    'dataset/label_mapping.pkl',\n",
        "    target_frames=TARGET_FRAMES,\n",
        "    training=True  # CÓ augmentation\n",
        ")\n",
        "\n",
        "val_dataset_base = VideoDataset(\n",
        "    'dataset/train',\n",
        "    'dataset/label_mapping.pkl',\n",
        "    target_frames=TARGET_FRAMES,\n",
        "    training=False  # KHÔNG augmentation\n",
        ")\n",
        "\n",
        "# Split\n",
        "train_size = int(0.8 * len(train_dataset_base))\n",
        "val_size = len(train_dataset_base) - train_size\n",
        "\n",
        "indices = list(range(len(train_dataset_base)))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:]\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(train_dataset_base, train_indices)\n",
        "val_dataset = torch.utils.data.Subset(val_dataset_base, val_indices)"
      ],
      "metadata": {
        "id": "eZmqjbQ1i7lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_sampler = create_balanced_sampler(train_dataset)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    sampler=balanced_sampler,  # ← THAY shuffle=True\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} (augmentation + balanced sampling)\")\n",
        "print(f\"Val: {len(val_dataset)} (no augmentation)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPmv0J3Ci3rD",
        "outputId": "eec2c847-a67e-47ea-d91a-3d69e2d35e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced Sampler: class counts min=4, max=59\n",
            "Train: 3100 (augmentation + balanced sampling)\n",
            "Val: 775 (no augmentation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNeXtTransformer(num_classes=NUM_CLASSES, hidden_size=256,\n",
        "             resnet_pretrained_weights=models.ResNet18_Weights.IMAGENET1K_V1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4THXnDMNiUGh",
        "outputId": "2a54a39c-b484-4024-910d-1bef4e302f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 109M/109M [00:00<00:00, 180MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=25,\n",
        "    lr=1e-4,\n",
        "    device='cuda',\n",
        "    save_path='augmented_balanced_convnexttransformer_best_model.pth'\n",
        ")"
      ],
      "metadata": {
        "id": "fhUNZidUiVJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export public result\n",
        "evaluate(\n",
        "    model=model,\n",
        "    folder_path=\"dataset/public_test\",\n",
        "    label_to_idx_path=\"dataset/label_mapping.pkl\",\n",
        "    model_path=\"augmented_balanced_convnexttransformer_best_model.pth\",\n",
        "    output_csv=\"public_test.csv\",\n",
        "    device=\"cuda\",\n",
        "    target_frames=16\n",
        ")"
      ],
      "metadata": {
        "id": "kSXHx7dZiWJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export public result\n",
        "evaluate(\n",
        "    model=model,\n",
        "    folder_path=\"dataset/private_test\",\n",
        "    label_to_idx_path=\"dataset/label_mapping.pkl\",\n",
        "    model_path=\"augmented_balanced_convnexttransformer_best_model.pth\",\n",
        "    output_csv=\"private_test.csv\",\n",
        "    device=\"cuda\",\n",
        "    target_frames=16\n",
        ")"
      ],
      "metadata": {
        "id": "GKvJ_lvkiZJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip final_augmented_balanced_convnexttransformer_submission.zip public_test.csv private_test.csv baseline.ipynb -j"
      ],
      "metadata": {
        "id": "ikpkUTgC0SNv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}