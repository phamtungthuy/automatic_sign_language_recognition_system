{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72beed9d-8044-4b25-b5e4-6e1245a5a3a2",
      "metadata": {
        "editable": true,
        "id": "72beed9d-8044-4b25-b5e4-6e1245a5a3a2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import csv\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "os.environ[\"TORCH_HOME\"] = '/opt/torch_models'\n",
        "\n",
        "# Constants\n",
        "NUM_CLASSES = 100\n",
        "TARGET_FRAMES = 16  # number of frames per video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e495320-ba1d-4f33-9393-92bdc8b06b23",
      "metadata": {
        "id": "5e495320-ba1d-4f33-9393-92bdc8b06b23"
      },
      "outputs": [],
      "source": [
        "# Read video frames using OpenCV\n",
        "def read_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(f\"Could not read any frames from {video_path}\")\n",
        "    frames = torch.from_numpy(np.stack(frames, axis=0))\n",
        "    return frames\n",
        "\n",
        "\n",
        "# Custom collate function for batching\n",
        "def collate_fn(batch):\n",
        "    frames = torch.stack([item['frames'] for item in batch])\n",
        "    labels = torch.tensor([item['label_idx'] for item in batch])\n",
        "    label_names = [item['label'] for item in batch]\n",
        "    return {'frames': frames, 'label_idx': labels, 'label': label_names}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edeebd7-9d31-47c8-8d3c-c8f59acedad9",
      "metadata": {
        "id": "1edeebd7-9d31-47c8-8d3c-c8f59acedad9"
      },
      "outputs": [],
      "source": [
        "# Define video dataset\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, label_to_idx_path, transform=None,\n",
        "                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n",
        "                 target_frames=32):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.mean, self.std = mean, std\n",
        "        self.target_frames = target_frames\n",
        "        self.instances, self.labels, self.label_idx = [], [], []\n",
        "\n",
        "        with open(label_to_idx_path, 'rb') as f:\n",
        "            self.label_mapping = pickle.load(f)\n",
        "\n",
        "        for label_folder in sorted(os.listdir(root_dir))[:NUM_CLASSES]:\n",
        "            path = os.path.join(root_dir, label_folder)\n",
        "            if os.path.isdir(path):\n",
        "                for video_file in os.listdir(path):\n",
        "                    video_path = os.path.join(path, video_file)\n",
        "                    self.instances.append(video_path)\n",
        "                    self.labels.append(label_folder)\n",
        "                    self.label_idx.append(self.label_mapping[label_folder])\n",
        "\n",
        "    # Downsample frames to fixed length\n",
        "    def _downsample_frames(self, frames):\n",
        "        num_frames = frames.shape[0]\n",
        "        if num_frames == self.target_frames:\n",
        "            return frames\n",
        "        elif num_frames < self.target_frames:\n",
        "            pad = self.target_frames - num_frames\n",
        "            return torch.cat([frames, frames[-1:].repeat(pad, 1, 1, 1)], dim=0)\n",
        "        else:\n",
        "            idx = torch.linspace(0, num_frames - 1, self.target_frames).long()\n",
        "            return frames[idx]\n",
        "\n",
        "    # Normalize frames with ImageNet stats\n",
        "    def _normalize(self, frames):\n",
        "        frames = frames.permute(0, 3, 1, 2).float() / 255.0\n",
        "        mean = torch.tensor(self.mean).view(1, 3, 1, 1)\n",
        "        std = torch.tensor(self.std).view(1, 3, 1, 1)\n",
        "        return (frames - mean) / std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.instances[idx]\n",
        "        label, label_idx = self.labels[idx], self.label_idx[idx]\n",
        "        frames = read_video(video_path)\n",
        "        frames = self._downsample_frames(frames)\n",
        "        frames = self._normalize(frames)\n",
        "        return {\"frames\": frames, \"label_idx\": label_idx, \"label\": label}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785ffbdd-5498-45c8-b900-55c95e60a15b",
      "metadata": {
        "id": "785ffbdd-5498-45c8-b900-55c95e60a15b"
      },
      "outputs": [],
      "source": [
        "# Define CRNN model\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, num_classes=100, hidden_size=256, resnet_pretrained_weights=None):\n",
        "        super(CRNN, self).__init__()\n",
        "        resnet = models.resnet18(weights=resnet_pretrained_weights)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.feature_dim = 512\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.rnn = nn.LSTM(self.feature_dim, hidden_size, batch_first=True, dropout=0.3)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.size()\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        features = self.cnn(x)\n",
        "        pooled = self.pool(features).squeeze(-1).squeeze(-1)\n",
        "        seq = pooled.view(B, T, self.feature_dim)\n",
        "        rnn_out, _ = self.rnn(seq)\n",
        "        final = rnn_out[:, -1, :]\n",
        "        return self.fc(final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c955a4ef-b38e-455a-90fc-a6d80df016c2",
      "metadata": {
        "id": "c955a4ef-b38e-455a-90fc-a6d80df016c2"
      },
      "outputs": [],
      "source": [
        "# One training epoch\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device='cuda'):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress = tqdm(dataloader, desc='Training')\n",
        "    for batch in progress:\n",
        "        frames, labels = batch['frames'].to(device), batch['label_idx'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        progress.set_postfix({'loss': f'{total_loss / (len(progress)+1e-9):.4f}'})\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# Validation\n",
        "def validate(model, dataloader, criterion, device='cuda'):\n",
        "    model.eval()\n",
        "    total_loss, preds, labels_all = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc='Validation'):\n",
        "            frames, labels = batch['frames'].to(device), batch['label_idx'].to(device)\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            preds.extend(predicted.cpu().numpy())\n",
        "            labels_all.extend(labels.cpu().numpy())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels_all, preds, average='macro', zero_division=0)\n",
        "    return total_loss / len(dataloader), {'precision': precision*100, 'recall': recall*100, 'f1': f1*100}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67294df5-309b-4073-b7c2-91ff59c7b2e0",
      "metadata": {
        "id": "67294df5-309b-4073-b7c2-91ff59c7b2e0"
      },
      "outputs": [],
      "source": [
        "# Full training loop with validation and test evaluation\n",
        "def train_model(model, train_loader, val_loader,\n",
        "                num_epochs=10, lr=5e-4, device='cuda', save_path='best_model.pth'):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_metrics = validate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Val F1: {val_metrics['f1']:.2f}% | Precision: {val_metrics['precision']:.2f}% | Recall: {val_metrics['recall']:.2f}%\")\n",
        "\n",
        "        if val_metrics['f1'] > best_f1:\n",
        "            best_f1 = val_metrics['f1']\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"✓ Best model saved with F1: {best_f1:.2f}%\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68fd3cd9-777f-45bc-b47f-e164d65ab0d1",
      "metadata": {
        "id": "68fd3cd9-777f-45bc-b47f-e164d65ab0d1",
        "outputId": "07b273ff-2b06-42f2-f0db-59f99bb97944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 3100, Val: 775\n",
            "\n",
            "===== Epoch 1/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:21<00:00,  4.60it/s, loss=4.1440]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 15.63% | Precision: 26.37% | Recall: 17.33%\n",
            "✓ Best model saved with F1: 15.63%\n",
            "\n",
            "===== Epoch 2/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:21<00:00,  4.50it/s, loss=2.9327]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 43.29% | Precision: 47.62% | Recall: 45.12%\n",
            "✓ Best model saved with F1: 43.29%\n",
            "\n",
            "===== Epoch 3/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:22<00:00,  4.30it/s, loss=1.8699]\n",
            "Validation: 100%|██████████| 25/25 [00:07<00:00,  3.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 56.43% | Precision: 59.38% | Recall: 58.49%\n",
            "✓ Best model saved with F1: 56.43%\n",
            "\n",
            "===== Epoch 4/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:18<00:00,  5.11it/s, loss=1.1245]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 65.69% | Precision: 71.39% | Recall: 66.49%\n",
            "✓ Best model saved with F1: 65.69%\n",
            "\n",
            "===== Epoch 5/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:24<00:00,  3.95it/s, loss=0.6970]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 68.45% | Precision: 71.87% | Recall: 68.82%\n",
            "✓ Best model saved with F1: 68.45%\n",
            "\n",
            "===== Epoch 6/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:23<00:00,  4.07it/s, loss=0.4300]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 75.29% | Precision: 78.97% | Recall: 75.12%\n",
            "✓ Best model saved with F1: 75.29%\n",
            "\n",
            "===== Epoch 7/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:19<00:00,  4.92it/s, loss=0.2700]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 78.42% | Precision: 82.16% | Recall: 77.78%\n",
            "✓ Best model saved with F1: 78.42%\n",
            "\n",
            "===== Epoch 8/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:19<00:00,  4.94it/s, loss=0.1792]\n",
            "Validation: 100%|██████████| 25/25 [00:06<00:00,  3.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 79.21% | Precision: 82.24% | Recall: 78.63%\n",
            "✓ Best model saved with F1: 79.21%\n",
            "\n",
            "===== Epoch 9/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:21<00:00,  4.57it/s, loss=0.1272]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 81.97% | Precision: 83.62% | Recall: 81.99%\n",
            "✓ Best model saved with F1: 81.97%\n",
            "\n",
            "===== Epoch 10/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:25<00:00,  3.86it/s, loss=0.0898]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 81.68% | Precision: 83.71% | Recall: 81.55%\n",
            "\n",
            "===== Epoch 11/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:21<00:00,  4.61it/s, loss=0.0653]\n",
            "Validation: 100%|██████████| 25/25 [00:06<00:00,  3.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 81.55% | Precision: 83.23% | Recall: 81.72%\n",
            "\n",
            "===== Epoch 12/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:22<00:00,  4.41it/s, loss=0.0494]\n",
            "Validation: 100%|██████████| 25/25 [00:07<00:00,  3.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 82.01% | Precision: 83.82% | Recall: 81.76%\n",
            "✓ Best model saved with F1: 82.01%\n",
            "\n",
            "===== Epoch 13/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:24<00:00,  4.02it/s, loss=0.0410]\n",
            "Validation: 100%|██████████| 25/25 [00:07<00:00,  3.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 80.88% | Precision: 82.45% | Recall: 81.06%\n",
            "\n",
            "===== Epoch 14/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:23<00:00,  4.10it/s, loss=0.0348]\n",
            "Validation: 100%|██████████| 25/25 [00:06<00:00,  3.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 82.69% | Precision: 84.09% | Recall: 82.51%\n",
            "✓ Best model saved with F1: 82.69%\n",
            "\n",
            "===== Epoch 15/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:20<00:00,  4.78it/s, loss=0.0300]\n",
            "Validation: 100%|██████████| 25/25 [00:06<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 81.82% | Precision: 83.32% | Recall: 81.86%\n",
            "\n",
            "===== Epoch 16/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:21<00:00,  4.49it/s, loss=0.0256]\n",
            "Validation: 100%|██████████| 25/25 [00:06<00:00,  3.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 81.64% | Precision: 83.02% | Recall: 81.71%\n",
            "\n",
            "===== Epoch 17/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:19<00:00,  4.88it/s, loss=0.0218]\n",
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 81.29% | Precision: 82.59% | Recall: 81.38%\n",
            "\n",
            "===== Epoch 18/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:20<00:00,  4.67it/s, loss=0.0192]\n",
            "Validation: 100%|██████████| 25/25 [00:06<00:00,  3.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 82.54% | Precision: 83.97% | Recall: 82.61%\n",
            "\n",
            "===== Epoch 19/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:26<00:00,  3.66it/s, loss=0.0170]\n",
            "Validation: 100%|██████████| 25/25 [00:06<00:00,  3.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 81.41% | Precision: 82.98% | Recall: 81.56%\n",
            "\n",
            "===== Epoch 20/20 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 97/97 [00:22<00:00,  4.37it/s, loss=0.0152]\n",
            "Validation: 100%|██████████| 25/25 [00:07<00:00,  3.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val F1: 81.17% | Precision: 82.37% | Recall: 81.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "model = CRNN(num_classes=NUM_CLASSES, hidden_size=256,\n",
        "             resnet_pretrained_weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "full_train = VideoDataset('dataset/train', 'dataset/label_mapping.pkl', target_frames=TARGET_FRAMES)\n",
        "train_size = int(0.8 * len(full_train))\n",
        "val_size = len(full_train) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
        "\n",
        "model = train_model(model, train_loader, val_loader,\n",
        "                    num_epochs=20, lr=1e-4, device='cuda', save_path='best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968e186f-1eee-4a8e-a67f-b56341b55e3a",
      "metadata": {
        "id": "968e186f-1eee-4a8e-a67f-b56341b55e3a"
      },
      "outputs": [],
      "source": [
        "# Evaluate trained model on test set\n",
        "def evaluate(model, folder_path, label_to_idx_path, output_csv=\"predictions.csv\",\n",
        "             device='cuda', model_path=None, target_frames=16):\n",
        "    # Load trained weights if provided\n",
        "    if model_path:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"Loaded model from {model_path}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load label mapping\n",
        "    with open(label_to_idx_path, 'rb') as f:\n",
        "        label_mapping = pickle.load(f)\n",
        "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "    # Collect video files\n",
        "    video_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))])\n",
        "    print(f\"Found {len(video_files)} videos in '{folder_path}'\")\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    dataset = VideoDataset(\n",
        "                  root_dir=folder_path,\n",
        "                  label_to_idx_path=label_to_idx_path,\n",
        "                  target_frames=target_frames\n",
        "              )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for video_file in tqdm(video_files, desc=\"Predicting\"):\n",
        "            video_path = os.path.join(folder_path, video_file)\n",
        "            try:\n",
        "                # Read and preprocess video\n",
        "                frames = read_video(video_path)\n",
        "                frames = dataset._downsample_frames(frames)\n",
        "                frames = dataset._normalize(frames)\n",
        "                frames = frames.unsqueeze(0).to(device)  # (1, T, C, H, W)\n",
        "\n",
        "                # Predict\n",
        "                outputs = model(frames)\n",
        "                _, predicted = outputs.max(1)\n",
        "                label_idx = predicted.item()\n",
        "                label_name = idx_to_label[label_idx]\n",
        "\n",
        "                predictions.append((video_file, label_name))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {video_file}: {e}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    with open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['video_name', 'label'])\n",
        "        writer.writerows(predictions)\n",
        "\n",
        "    print(f\"\\nPredictions saved to '{output_csv}'\")\n",
        "    print(f\"Total videos processed: {len(predictions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac1962d-8553-4167-aea8-2b0c4dfdab83",
      "metadata": {
        "id": "8ac1962d-8553-4167-aea8-2b0c4dfdab83",
        "outputId": "c3d0b331-416d-4805-e33a-df42505a55f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from best_model.pth\n",
            "Found 1630 videos in 'dataset/public_test'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 1630/1630 [02:42<00:00, 10.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions saved to 'public_submission.csv'\n",
            "Total videos processed: 1630\n",
            "Created file public_submission.zip successfully.\n"
          ]
        }
      ],
      "source": [
        "# Export public result\n",
        "evaluate(\n",
        "    model=model,\n",
        "    folder_path=\"dataset/public_test\",\n",
        "    label_to_idx_path=\"dataset/label_mapping.pkl\",\n",
        "    model_path=\"best_model.pth\",\n",
        "    output_csv=\"public_submission.csv\",\n",
        "    device=\"cuda\",\n",
        "    target_frames=16\n",
        ")\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"public_submission.zip\", 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipf.write(\"public_submission.csv\")\n",
        "        print(\"Created file public_submission.zip successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e97765e-9db0-4759-a8a5-a08997db17bf",
      "metadata": {
        "id": "3e97765e-9db0-4759-a8a5-a08997db17bf"
      },
      "outputs": [],
      "source": [
        "# Export private result\n",
        "evaluate(\n",
        "    model=model,\n",
        "    folder_path=\"dataset/private_test\",\n",
        "    label_to_idx_path=\"dataset/label_mapping.pkl\",\n",
        "    model_path=\"best_model.pth\",\n",
        "    output_csv=\"private_submission.csv\",\n",
        "    device=\"cuda\",\n",
        "    target_frames=16\n",
        ")\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"private_test.zip\", 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipf.write(\"private_submission.csv\")\n",
        "        print(\"Created file private_submission.zip successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}